--- For this experiment, we trained the algorithms with the following configuration --- 
Dataset loaded :  datasets/Mar_set.csv
NIT criterion data used for training:  True
Criteria data used for training :  True
Action data used for training:  True
Post-criteria data used for training:  False
Computed metric:  f1-score
--- ---
\multicolumn{1}{|l|}{\textbf{Logistic Regression}} & $0.605 \pm 0.161$ & $0.614 \pm 0.139$ & $0.534 \pm 0.128$ & $0.492 \pm 0.092$ \\ \hline
\multicolumn{1}{|l|}{\textbf{KNN}} & $0.49 \pm 0.085$ & $0.63 \pm 0.144$ & $0.55 \pm 0.092$ & $0.299 \pm 0.077$ \\ \hline
\multicolumn{1}{|l|}{\textbf{SVC with linear kernel}} & $0.268 \pm 0.054$ & $0.334 \pm 0.104$ & $0.487 \pm 0.119$ & $0.349 \pm 0.087$ \\ \hline
\multicolumn{1}{|l|}{\textbf{SVC with rbf kernel}} & $0.406 \pm 0.131$ & $0.273 \pm 0.074$ & $0.319 \pm 0.102$ & $0.222 \pm 0.054$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Gaussian Process}} & $0.588 \pm 0.199$ & $0.541 \pm 0.121$ & $0.504 \pm 0.088$ & $0.503 \pm 0.06$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Decision Tree}} & $0.528 \pm 0.173$ & $0.488 \pm 0.14$ & $0.488 \pm 0.085$ & $0.471 \pm 0.078$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Random forest}} & $0.506 \pm 0.143$ & $0.494 \pm 0.116$ & $0.56 \pm 0.13$ & $0.461 \pm 0.116$ \\ \hline
\multicolumn{1}{|l|}{\textbf{MLP}} & $0.579 \pm 0.205$ & $0.581 \pm 0.108$ & $0.604 \pm 0.144$ & $0.518 \pm 0.103$ \\ \hline
\multicolumn{1}{|l|}{\textbf{QDA}} & $0.462 \pm 0.119$ & $0.452 \pm 0.089$ & $0.455 \pm 0.102$ & $0.254 \pm 0.1$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Gradient Boosting}} & $0.502 \pm 0.1$ & $0.552 \pm 0.137$ & $0.629 \pm 0.131$ & $0.509 \pm 0.102$ \\ \hline
\multicolumn{1}{|l|}{\textbf{XG Boosting}} & $0.574 \pm 0.172$ & $0.556 \pm 0.159$ & $0.59 \pm 0.136$ & $0.505 \pm 0.11$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Random dummy}} & $0.251 \pm 0.075$ & $0.289 \pm 0.09$ & $0.347 \pm 0.113$ & $0.312 \pm 0.122$ \\ \hline
