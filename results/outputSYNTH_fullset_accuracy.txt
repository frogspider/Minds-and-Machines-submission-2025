--- For this experiment, we trained the algorithms with the following configuration --- 
Dataset loaded :  datasets/Synth_set.csv
NIT criterion data used for training:  True
Criteria data used for training :  True
Action data used for training:  True
Post-criteria data used for training:  False
Computed metric:  accuracy
--- ---
\multicolumn{1}{|l|}{\textbf{Logistic Regression}} & $0.781 \pm 0.037$ & $0.689 \pm 0.045$ & $0.941 \pm 0.018$ & $1.0 \pm 0.0$ \\ \hline
\multicolumn{1}{|l|}{\textbf{KNN}} & $0.558 \pm 0.034$ & $0.535 \pm 0.029$ & $0.571 \pm 0.04$ & $0.836 \pm 0.026$ \\ \hline
\multicolumn{1}{|l|}{\textbf{SVC with linear kernel}} & $0.722 \pm 0.024$ & $0.684 \pm 0.031$ & $0.863 \pm 0.032$ & $0.736 \pm 0.041$ \\ \hline
\multicolumn{1}{|l|}{\textbf{SVC with rbf kernel}} & $0.444 \pm 0.064$ & $0.386 \pm 0.052$ & $0.412 \pm 0.04$ & $0.617 \pm 0.078$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Gaussian Process}} & $0.866 \pm 0.026$ & $0.805 \pm 0.028$ & $0.958 \pm 0.017$ & $1.0 \pm 0.0$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Decision Tree}} & $0.798 \pm 0.037$ & $0.688 \pm 0.036$ & $0.568 \pm 0.033$ & $1.0 \pm 0.0$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Random forest}} & $0.598 \pm 0.033$ & $0.581 \pm 0.032$ & $0.561 \pm 0.035$ & $0.747 \pm 0.045$ \\ \hline
\multicolumn{1}{|l|}{\textbf{MLP}} & $0.908 \pm 0.022$ & $0.862 \pm 0.024$ & $0.953 \pm 0.019$ & $1.0 \pm 0.0$ \\ \hline
\multicolumn{1}{|l|}{\textbf{QDA}} & $0.718 \pm 0.04$ & $0.666 \pm 0.035$ & $0.873 \pm 0.031$ & $0.998 \pm 0.005$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Gradient Boosting}} & $0.929 \pm 0.019$ & $0.855 \pm 0.026$ & $0.811 \pm 0.032$ & $1.0 \pm 0.0$ \\ \hline
\multicolumn{1}{|l|}{\textbf{XG Boosting}} & $0.933 \pm 0.018$ & $0.884 \pm 0.02$ & $0.778 \pm 0.028$ & $1.0 \pm 0.0$ \\ \hline
\multicolumn{1}{|l|}{\textbf{Random dummy}} & $0.334 \pm 0.031$ & $0.337 \pm 0.041$ & $0.317 \pm 0.039$ & $0.308 \pm 0.047$ \\ \hline
